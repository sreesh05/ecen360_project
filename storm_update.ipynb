{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2ca45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Injuries/fatalities #Jase\n",
    "#Begin and End Long and Lat #Jase\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('StormEvents_details-ftp_v1 (version 2).xlsb.csv')\n",
    "\n",
    "fatalities_texas = df[(df['DEATHS_DIRECT'] > 0) & (df['STATE'] == 'TEXAS')]\n",
    "\n",
    "fatalities_texas = fatalities_texas[['EVENT_ID', 'DEATHS_DIRECT']]\n",
    "\n",
    "fatalities_texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc3fc2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_texas = df[df['STATE'] == 'TEXAS']\n",
    "\n",
    "injuries_indirect_texas = df_texas[df_texas['INJURIES_INDIRECT'] > 0]['INJURIES_INDIRECT'].sum()\n",
    "injuries_direct_texas = df_texas[df_texas['INJURIES_DIRECT'] > 0]['INJURIES_DIRECT'].sum()\n",
    "fatalities_direct_texas = df_texas[df_texas['DEATHS_DIRECT'] > 0]['DEATHS_DIRECT'].sum()\n",
    "fatalities_indirect_texas = df_texas[df_texas['DEATHS_INDIRECT'] > 0]['DEATHS_INDIRECT'].sum()\n",
    "\n",
    "data = [injuries_indirect_texas, injuries_direct_texas, fatalities_direct_texas, fatalities_indirect_texas]\n",
    "\n",
    "labels = ['Indirect Injuries', 'Direct Injuries', 'Direct Fatalities', 'Indirect Fatalities']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(data, labels=labels, autopct='%1.1f%%')  \n",
    "ax.set_title('Harmful Storms in Texas 2024')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea4bf5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Test for the other years\n",
    "df = data_2020\n",
    "df_texas = df[df['STATE'] == 'TEXAS']\n",
    "\n",
    "injuries_indirect_texas = df_texas[df_texas['INJURIES_INDIRECT'] > 0]['INJURIES_INDIRECT'].sum()\n",
    "injuries_direct_texas = df_texas[df_texas['INJURIES_DIRECT'] > 0]['INJURIES_DIRECT'].sum()\n",
    "fatalities_direct_texas = df_texas[df_texas['DEATHS_DIRECT'] > 0]['DEATHS_DIRECT'].sum()\n",
    "fatalities_indirect_texas = df_texas[df_texas['DEATHS_INDIRECT'] > 0]['DEATHS_INDIRECT'].sum()\n",
    "\n",
    "data = [injuries_indirect_texas, injuries_direct_texas, fatalities_direct_texas, fatalities_indirect_texas]\n",
    "\n",
    "labels = ['Indirect Injuries', 'Direct Injuries', 'Direct Fatalities', 'Indirect Fatalities']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(data, labels=labels, autopct='%1.1f%%')  \n",
    "ax.set_title('Harmful Storms in Texas 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acf84e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('StormEvents_details-ftp_v1 (version 2).xlsb.csv')\n",
    "\n",
    "texas_storms = df[df['STATE'] == 'TEXAS']\n",
    "\n",
    "texas_storms = texas_storms.dropna(subset=['BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'])\n",
    "\n",
    "m = folium.Map(location=[31.9686, -99.9018], zoom_start=6)  \n",
    "\n",
    "for index, row in texas_storms.iterrows():\n",
    "    start_lat = row['BEGIN_LAT']\n",
    "    start_lon = row['BEGIN_LON']\n",
    "    end_lat = row['END_LAT']\n",
    "    end_lon = row['END_LON']\n",
    "\n",
    "    #folium.Marker([start_lat, start_lon], popup=f\"Start: {row['EVENT_ID']}\").add_to(m)\n",
    "    #folium.Marker([end_lat, end_lon], popup=f\"End: {row['EVENT_ID']}\").add_to(m)\n",
    "\n",
    "    folium.PolyLine(\n",
    "        locations=[[start_lat, start_lon], [end_lat, end_lon]],\n",
    "        color='blue', weight=2, opacity=0.6\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save('texas_storms_map.html')\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4673f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "data_2024 = data_2024[data_2024[\"STATE\"] == \"TEXAS\"]\n",
    "\n",
    "data_2024[\"BEGIN_LAT\"] = pd.to_numeric(data_2024[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2024[\"BEGIN_LON\"] = pd.to_numeric(data_2024[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2024 = data_2024.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "heat_data = [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2024.iterrows()]\n",
    "\n",
    "texas_center = [31.9686, -99.9018]  \n",
    "texas_map = folium.Map(location=texas_center, zoom_start=6)\n",
    "\n",
    "HeatMap(heat_data, radius=10, blur=15, min_opacity=0.8).add_to(texas_map)\n",
    "\n",
    "texas_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45b0db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_2024 = data_2024[data_2024[\"STATE\"] == \"TEXAS\"]\n",
    "data_2023 = data_2023[data_2023[\"STATE\"] == \"TEXAS\"]\n",
    "data_2022 = data_2022[data_2022[\"STATE\"] == \"TEXAS\"]\n",
    "data_2021 = data_2021[data_2021[\"STATE\"] == \"TEXAS\"]\n",
    "data_2020 = data_2020[data_2020[\"STATE\"] == \"TEXAS\"]\n",
    "data_2019 = data_2019[data_2019[\"STATE\"] == \"TEXAS\"]\n",
    "data_2018 = data_2018[data_2018[\"STATE\"] == \"TEXAS\"]\n",
    "data_2017 = data_2017[data_2017[\"STATE\"] == \"TEXAS\"]\n",
    "data_2016 = data_2016[data_2016[\"STATE\"] == \"TEXAS\"]\n",
    "data_2015 = data_2015[data_2015[\"STATE\"] == \"TEXAS\"]\n",
    "data_2014 = data_2014[data_2014[\"STATE\"] == \"TEXAS\"]\n",
    "data_2013 = data_2013[data_2013[\"STATE\"] == \"TEXAS\"]\n",
    "data_2012 = data_2012[data_2012[\"STATE\"] == \"TEXAS\"]\n",
    "data_2011 = data_2011[data_2011[\"STATE\"] == \"TEXAS\"]\n",
    "data_2010 = data_2010[data_2010[\"STATE\"] == \"TEXAS\"]\n",
    "data_2009 = data_2009[data_2009[\"STATE\"] == \"TEXAS\"]\n",
    "data_2008 = data_2008[data_2008[\"STATE\"] == \"TEXAS\"]\n",
    "data_2007 = data_2007[data_2007[\"STATE\"] == \"TEXAS\"]\n",
    "data_2006 = data_2006[data_2006[\"STATE\"] == \"TEXAS\"]\n",
    "data_2005 = data_2005[data_2005[\"STATE\"] == \"TEXAS\"]\n",
    "data_2004 = data_2004[data_2004[\"STATE\"] == \"TEXAS\"]\n",
    "data_2003 = data_2003[data_2003[\"STATE\"] == \"TEXAS\"]\n",
    "data_2002 = data_2002[data_2002[\"STATE\"] == \"TEXAS\"]\n",
    "\n",
    "\n",
    "data_2024[\"BEGIN_LAT\"] = pd.to_numeric(data_2024[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2024[\"BEGIN_LON\"] = pd.to_numeric(data_2024[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2024 = data_2024.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2023[\"BEGIN_LAT\"] = pd.to_numeric(data_2023[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2023[\"BEGIN_LON\"] = pd.to_numeric(data_2023[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2023 = data_2023.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2022[\"BEGIN_LAT\"] = pd.to_numeric(data_2022[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2022[\"BEGIN_LON\"] = pd.to_numeric(data_2022[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2022 = data_2022.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2021[\"BEGIN_LAT\"] = pd.to_numeric(data_2021[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2021[\"BEGIN_LON\"] = pd.to_numeric(data_2021[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2021 = data_2021.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2020[\"BEGIN_LAT\"] = pd.to_numeric(data_2020[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2020[\"BEGIN_LON\"] = pd.to_numeric(data_2020[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2020 = data_2020.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2019[\"BEGIN_LAT\"] = pd.to_numeric(data_2019[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2019[\"BEGIN_LON\"] = pd.to_numeric(data_2019[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2019 = data_2019.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2018[\"BEGIN_LAT\"] = pd.to_numeric(data_2018[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2018[\"BEGIN_LON\"] = pd.to_numeric(data_2018[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2018 = data_2018.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2017[\"BEGIN_LAT\"] = pd.to_numeric(data_2017[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2017[\"BEGIN_LON\"] = pd.to_numeric(data_2017[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2017 = data_2017.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2016[\"BEGIN_LAT\"] = pd.to_numeric(data_2016[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2016[\"BEGIN_LON\"] = pd.to_numeric(data_2016[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2016 = data_2016.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2015[\"BEGIN_LAT\"] = pd.to_numeric(data_2015[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2015[\"BEGIN_LON\"] = pd.to_numeric(data_2015[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2015 = data_2015.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2014[\"BEGIN_LAT\"] = pd.to_numeric(data_2014[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2014[\"BEGIN_LON\"] = pd.to_numeric(data_2014[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2014 = data_2014.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2013[\"BEGIN_LAT\"] = pd.to_numeric(data_2013[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2013[\"BEGIN_LON\"] = pd.to_numeric(data_2013[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2013 = data_2013.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2012[\"BEGIN_LAT\"] = pd.to_numeric(data_2012[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2012[\"BEGIN_LON\"] = pd.to_numeric(data_2012[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2012 = data_2012.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2011[\"BEGIN_LAT\"] = pd.to_numeric(data_2011[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2011[\"BEGIN_LON\"] = pd.to_numeric(data_2011[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2011 = data_2011.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2010[\"BEGIN_LAT\"] = pd.to_numeric(data_2010[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2010[\"BEGIN_LON\"] = pd.to_numeric(data_2010[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2010 = data_2010.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2009[\"BEGIN_LAT\"] = pd.to_numeric(data_2009[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2009[\"BEGIN_LON\"] = pd.to_numeric(data_2009[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2009 = data_2009.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2008[\"BEGIN_LAT\"] = pd.to_numeric(data_2008[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2008[\"BEGIN_LON\"] = pd.to_numeric(data_2008[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2008 = data_2008.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2007[\"BEGIN_LAT\"] = pd.to_numeric(data_2007[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2007[\"BEGIN_LON\"] = pd.to_numeric(data_2007[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2007 = data_2007.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2006[\"BEGIN_LAT\"] = pd.to_numeric(data_2006[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2006[\"BEGIN_LON\"] = pd.to_numeric(data_2006[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2006 = data_2006.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2005[\"BEGIN_LAT\"] = pd.to_numeric(data_2005[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2005[\"BEGIN_LON\"] = pd.to_numeric(data_2005[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2005 = data_2005.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2004[\"BEGIN_LAT\"] = pd.to_numeric(data_2004[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2004[\"BEGIN_LON\"] = pd.to_numeric(data_2004[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2004 = data_2004.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2003[\"BEGIN_LAT\"] = pd.to_numeric(data_2003[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2003[\"BEGIN_LON\"] = pd.to_numeric(data_2003[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2003 = data_2003.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "data_2002[\"BEGIN_LAT\"] = pd.to_numeric(data_2002[\"BEGIN_LAT\"], errors=\"coerce\")\n",
    "data_2002[\"BEGIN_LON\"] = pd.to_numeric(data_2002[\"BEGIN_LON\"], errors=\"coerce\")\n",
    "data_2002 = data_2002.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "heat_data = [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2024.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2023.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2022.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2021.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2020.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2019.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2018.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2017.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2016.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2015.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2014.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2013.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2012.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2011.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2010.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2009.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2008.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2007.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2006.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2005.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2004.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2003.iterrows()]\n",
    "heat_data += [(row[\"BEGIN_LAT\"], row[\"BEGIN_LON\"]) for _, row in data_2002.iterrows()]\n",
    "\n",
    "texas_center = [31.9686, -99.9018]  \n",
    "texas_map = folium.Map(location=texas_center, zoom_start=6)\n",
    "\n",
    "HeatMap(heat_data, radius=7, blur=5, min_opacity=0.8).add_to(texas_map)\n",
    "\n",
    "texas_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5226b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "df_texas = pd.concat(storm_data.values(), ignore_index=True)\n",
    "\n",
    "possible_date_cols = [col for col in df_texas.columns if 'DATE' in col.upper()]\n",
    "date_col = possible_date_cols[0]\n",
    "df_texas['YEAR'] = pd.to_datetime(df_texas[date_col], errors='coerce').dt.year\n",
    "df_texas = df_texas.dropna(subset=['YEAR', 'BEGIN_LAT', 'BEGIN_LON'])\n",
    "df_texas['YEAR'] = df_texas['YEAR'].astype(int)\n",
    "\n",
    "df_texas = df_texas[\n",
    "    (df_texas['BEGIN_LAT'] >= 25.8) & (df_texas['BEGIN_LAT'] <= 36.5) &\n",
    "    (df_texas['BEGIN_LON'] >= -106.7) & (df_texas['BEGIN_LON'] <= -93.5)\n",
    "]\n",
    "\n",
    "storm_counts = df_texas.groupby('YEAR').size()\n",
    "\n",
    "model = ARIMA(storm_counts, order=(2, 1, 1))\n",
    "model_fit = model.fit()\n",
    "future_years = np.arange(storm_counts.index.max() + 1, storm_counts.index.max() + 6)\n",
    "forecast = model_fit.forecast(steps=5).astype(int)\n",
    "\n",
    "historical_coords = df_texas[['BEGIN_LAT', 'BEGIN_LON']].dropna().values\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[31.0, -99.0],\n",
    "    zoom_start=6,\n",
    "    tiles='CartoDB dark_matter',\n",
    "    width='100%', \n",
    "    height='800px'  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HeatMap(historical_coords, name='Historical Storms').add_to(m)\n",
    "\n",
    "for year, count in zip(future_years, forecast):\n",
    "    if count > 0:\n",
    "        sampled = historical_coords[np.random.choice(historical_coords.shape[0], size=count)]\n",
    "        HeatMap(sampled, name=f'Predicted {year}', radius=5, blur=7).add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m.save('texas_storm_forecast_map.html')\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143bf4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "data_2002 = pd.read_csv('StormEvents_details-ftp_v1.0_d2002_c20220425.csv')\n",
    "data_2003 = pd.read_csv('StormEvents_details-ftp_v1.0_d2003_c20220425.csv')\n",
    "data_2004 = pd.read_csv('StormEvents_details-ftp_v1.0_d2004_c20220425.csv')\n",
    "data_2005 = pd.read_csv('StormEvents_details-ftp_v1.0_d2005_c20220425.csv')\n",
    "data_2006 = pd.read_csv('StormEvents_details-ftp_v1.0_d2006_c20250122.csv')\n",
    "data_2007 = pd.read_csv('StormEvents_details-ftp_v1.0_d2007_c20240216.csv')\n",
    "data_2008 = pd.read_csv('StormEvents_details-ftp_v1.0_d2008_c20240620.csv')\n",
    "data_2009 = pd.read_csv('StormEvents_details-ftp_v1.0_d2009_c20231116.csv')\n",
    "data_2010 = pd.read_csv('StormEvents_details-ftp_v1.0_d2010_c20220425.csv')\n",
    "data_2011 = pd.read_csv('StormEvents_details-ftp_v1.0_d2011_c20230417.csv')\n",
    "data_2012 = pd.read_csv('StormEvents_details-ftp_v1.0_d2012_c20221216.csv')\n",
    "data_2013 = pd.read_csv('StormEvents_details-ftp_v1.0_d2013_c20230118.csv')\n",
    "data_2014 = pd.read_csv('StormEvents_details-ftp_v1.0_d2014_c20231116.csv')\n",
    "data_2015 = pd.read_csv('StormEvents_details-ftp_v1.0_d2015_c20240716.csv')\n",
    "data_2016 = pd.read_csv('StormEvents_details-ftp_v1.0_d2016_c20220719.csv')\n",
    "data_2017 = pd.read_csv('StormEvents_details-ftp_v1.0_d2017_c20250122.csv')\n",
    "data_2018 = pd.read_csv('StormEvents_details-ftp_v1.0_d2018_c20240716.csv')\n",
    "data_2019 = pd.read_csv('StormEvents_details-ftp_v1.0_d2019_c20240117.csv')\n",
    "data_2020 = pd.read_csv('StormEvents_details-ftp_v1.0_d2020_c20240620.csv')\n",
    "data_2021 = pd.read_csv('StormEvents_details-ftp_v1.0_d2021_c20240716.csv')\n",
    "data_2022 = pd.read_csv('StormEvents_details-ftp_v1.0_d2022_c20241121.csv')\n",
    "data_2023 = pd.read_csv('StormEvents_details-ftp_v1.0_d2023_c20250317.csv')\n",
    "data_2024 = pd.read_csv('StormEvents_details-ftp_v1 (version 2).xlsb.csv')\n",
    "\n",
    "# Assuming you have already loaded your storm data for each year into variables like data_2002, data_2003, etc.\n",
    "storm_data = {\n",
    "    2002: data_2002, 2003: data_2003, 2004: data_2004, 2005: data_2005,\n",
    "    2006: data_2006, 2007: data_2007, 2008: data_2008, 2009: data_2009,\n",
    "    2010: data_2010, 2011: data_2011, 2012: data_2012, 2013: data_2013,\n",
    "    2014: data_2014, 2015: data_2015, 2016: data_2016, 2017: data_2017,\n",
    "    2018: data_2018, 2019: data_2019, 2020: data_2020, 2021: data_2021,\n",
    "    2022: data_2022, 2023: data_2023, 2024: data_2024\n",
    "}\n",
    "\n",
    "# Create the base map of Texas\n",
    "# Create the base map of Texas\n",
    "texas_map = folium.Map(location=[31.9686, -99.9018], zoom_start=6)\n",
    "\n",
    "# Create a MarkerCluster to group storm markers\n",
    "marker_cluster = MarkerCluster().add_to(texas_map)\n",
    "\n",
    "# Loop through each year in your storm_data\n",
    "for year, df in storm_data.items():\n",
    "    # Filter only Texas data (assuming the 'STATE' column contains the state info)\n",
    "    df_texas = df[df['STATE'] == 'TEXAS']\n",
    "    \n",
    "    # Loop through each storm event in Texas and plot on the map\n",
    "    for idx, row in df_texas.iterrows():\n",
    "        latitude = row['BEGIN_LAT']\n",
    "        longitude = row['BEGIN_LON']\n",
    "        \n",
    "        # Skip rows with invalid latitude or longitude\n",
    "        if pd.isna(latitude) or pd.isna(longitude):\n",
    "            continue\n",
    "        \n",
    "        deaths = row['DEATHS_DIRECT']\n",
    "        injuries = row['INJURIES_DIRECT']\n",
    "        event_type = row['EVENT_TYPE']\n",
    "        \n",
    "        # Add a marker with a popup showing the event info (deaths, injuries, etc.)\n",
    "        folium.Marker(\n",
    "            location=[latitude, longitude],\n",
    "            popup=f\"Year: {year}<br>Event Type: {event_type}<br>Deaths: {deaths}<br>Injuries: {injuries}\",\n",
    "            icon=folium.Icon(color='red' if deaths > 0 else 'blue')\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "texas_map.save(\"texas_storms_map.html\")\n",
    "texas_map\n",
    "# Total Deaths per Year (with markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3d89e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define latitude bands (these are approximations)\n",
    "def classify_latitude_band(lat):\n",
    "    if lat < 30:\n",
    "        return 'South Texas'\n",
    "    elif 30 <= lat < 33:\n",
    "        return 'Central Texas'\n",
    "    else:\n",
    "        return 'North Texas'\n",
    "\n",
    "# Build a DataFrame of all storms with latitudes and years\n",
    "lat_band_data = []\n",
    "\n",
    "for year, df in storm_data.items():\n",
    "    df_texas = df[df['STATE'] == 'TEXAS']\n",
    "    df_valid = df_texas[pd.notna(df_texas['BEGIN_LAT'])]\n",
    "\n",
    "    for idx, row in df_valid.iterrows():\n",
    "        lat = row['BEGIN_LAT']\n",
    "        band = classify_latitude_band(lat)\n",
    "        lat_band_data.append({'Year': year, 'Band': band})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "lat_band_df = pd.DataFrame(lat_band_data)\n",
    "\n",
    "# Group and count storm events by year and latitude band\n",
    "lat_band_summary = lat_band_df.groupby(['Year', 'Band']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot the trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "lat_band_summary.plot(marker='o')\n",
    "plt.title(\"Storm Frequency by Latitude Band (2002–2024)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Storms\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Region\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd00b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Prepare the data\n",
    "fatalities_data = []\n",
    "for year, df in storm_data.items():\n",
    "    df_texas = df[df['STATE'] == 'TEXAS']\n",
    "    total_fatalities = df_texas['DEATHS_DIRECT'].sum() + df_texas['DEATHS_INDIRECT'].sum()\n",
    "    fatalities_data.append({'Year': year, 'Fatalities': total_fatalities})\n",
    "\n",
    "fatalities_df = pd.DataFrame(fatalities_data).set_index('Year')\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(fatalities_df['Fatalities'], order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Align actual values with fitted values\n",
    "fitted_values = fitted_model.fittedvalues\n",
    "actual = fatalities_df['Fatalities'].iloc[-len(fitted_values):]  # Trim actual to match fitted\n",
    "r2 = r2_score(actual, fitted_values)\n",
    "\n",
    "# Forecast future values\n",
    "forecast_steps = 10\n",
    "forecast = fitted_model.forecast(steps=forecast_steps)\n",
    "forecast_years = list(range(2025, 2025 + forecast_steps))\n",
    "forecast_df = pd.DataFrame({'Year': forecast_years, 'Predicted Fatalities': forecast.values})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fatalities_df.index, fatalities_df['Fatalities'], label='Historical Fatalities', marker='o')\n",
    "plt.plot(fitted_values.index, fitted_values, label='In-sample Prediction', linestyle='--', color='green')\n",
    "plt.plot(forecast_df['Year'], forecast_df['Predicted Fatalities'], label='Forecast (2025–2034)', linestyle='--', marker='x', color='orange')\n",
    "plt.title(f'Texas Storm Fatalities with ARIMA Forecast\\nIn-sample $R^2$: {r2:.3f}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Fatalities')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
